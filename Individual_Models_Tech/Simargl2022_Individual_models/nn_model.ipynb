{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_744746/3441968052.py:9: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/tmp/ipykernel_744746/3441968052.py:17: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/home/ibibers@ads.iu.edu/IDS_Datasets/Combined_datasets/Simargl2022_combined_dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "# @Time : 2024/2/5 16:00\n",
    "# @Author : Ismail Bibers\n",
    "# This code is written in Python 3.7. To install the required packages, run the following commands:\n",
    "# pip install pandas numpy matplotlib seaborn scikit-learn sympy\n",
    "# This code is applicable to the Simargl 2022 dataset. \n",
    "# implemented Decision Tree Classifier. \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/home/ibibers@ads.iu.edu/IDS_Datasets/Combined_datasets/Simargl2022_combined_dataset.csv')\n",
    "\n",
    "# Drop duplicates: same observation but multiple occurences\n",
    "main_df_1 = df.drop_duplicates(keep='first')\n",
    "\n",
    "# Drop columns that have just one unique value, the model wont learn form columns like this.\n",
    "one_value = main_df_1.columns[main_df_1.nunique() == 1]\n",
    "main_df_2 = main_df_1.drop(columns = one_value, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take random samples of the dataset\n",
    "# sample_df_1 = main_df_2.sample(frac=0.6, random_state=42)\n",
    "sample_df_1 = main_df_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extract subsample of data: \n",
      "ALERT\n",
      "Denial of Service    5138973\n",
      "Port Scanning        4170194\n",
      "Malware                  571\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract subsample of data\n",
    "print (\" Extract subsample of data: \")\n",
    "print(sample_df_1['ALERT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     FLOW_ID PROTOCOL_MAP  L4_SRC_PORT   IPV4_SRC_ADDR  L4_DST_PORT  \\\n",
      "0  334626451          tcp        50242  10.114.241.191          443   \n",
      "1  334626948          tcp        50244  10.114.241.191          443   \n",
      "2  334627641          tcp        50246  10.114.241.191          443   \n",
      "3  334628195          tcp        50248  10.114.241.191          443   \n",
      "4  334628673          tcp        54328  10.114.241.191          443   \n",
      "\n",
      "   IPV4_DST_ADDR  FIRST_SWITCHED  FLOW_DURATION_MILLISECONDS  LAST_SWITCHED  \\\n",
      "0  10.114.224.73      1647344604                         105     1647344604   \n",
      "1  10.114.224.73      1647344609                           6     1647344609   \n",
      "2  10.114.224.73      1647344614                         111     1647344614   \n",
      "3  10.114.224.73      1647344619                           6     1647344619   \n",
      "4  10.114.224.73      1647344534                      110000     1647344644   \n",
      "\n",
      "   PROTOCOL  ...  SRC_TOS  DST_TOS  TOTAL_FLOWS_EXP  IN_BYTES  IN_PKTS  \\\n",
      "0         6  ...        0        0        334626451       551        5   \n",
      "1         6  ...        0        0        334626948       551        5   \n",
      "2         6  ...        0        0        334627641       551        5   \n",
      "3         6  ...        0        0        334628195       551        5   \n",
      "4         6  ...        0        0        334628673      1309       12   \n",
      "\n",
      "   OUT_BYTES  OUT_PKTS              ALERT  ANALYSIS_TIMESTAMP  ANOMALY  \n",
      "0        192         4  Denial of Service          1647344695      NaN  \n",
      "1        192         4  Denial of Service          1647344695      NaN  \n",
      "2        192         4  Denial of Service          1647344695      NaN  \n",
      "3        192         4  Denial of Service          1647344695      NaN  \n",
      "4        624        12  Denial of Service          1647344695      NaN  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Git the features\n",
    "print (main_df_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24359068 entries, 0 to 30148257\n",
      "Data columns (total 28 columns):\n",
      " #   Column                      Dtype  \n",
      "---  ------                      -----  \n",
      " 0   FLOW_ID                     int64  \n",
      " 1   PROTOCOL_MAP                object \n",
      " 2   L4_SRC_PORT                 int64  \n",
      " 3   IPV4_SRC_ADDR               object \n",
      " 4   L4_DST_PORT                 int64  \n",
      " 5   IPV4_DST_ADDR               object \n",
      " 6   FIRST_SWITCHED              int64  \n",
      " 7   FLOW_DURATION_MILLISECONDS  int64  \n",
      " 8   LAST_SWITCHED               int64  \n",
      " 9   PROTOCOL                    int64  \n",
      " 10  TCP_FLAGS                   int64  \n",
      " 11  TCP_WIN_MAX_IN              int64  \n",
      " 12  TCP_WIN_MAX_OUT             int64  \n",
      " 13  TCP_WIN_MIN_IN              int64  \n",
      " 14  TCP_WIN_MIN_OUT             int64  \n",
      " 15  TCP_WIN_MSS_IN              int64  \n",
      " 16  TCP_WIN_SCALE_IN            int64  \n",
      " 17  TCP_WIN_SCALE_OUT           int64  \n",
      " 18  SRC_TOS                     int64  \n",
      " 19  DST_TOS                     int64  \n",
      " 20  TOTAL_FLOWS_EXP             int64  \n",
      " 21  IN_BYTES                    int64  \n",
      " 22  IN_PKTS                     int64  \n",
      " 23  OUT_BYTES                   int64  \n",
      " 24  OUT_PKTS                    int64  \n",
      " 25  ALERT                       object \n",
      " 26  ANALYSIS_TIMESTAMP          int64  \n",
      " 27  ANOMALY                     float64\n",
      "dtypes: float64(1), int64(23), object(4)\n",
      "memory usage: 5.3+ GB\n"
     ]
    }
   ],
   "source": [
    "sample_df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To see the features that are need to be encoded \"Which are not numerical\"\n",
    "# categorical_columns = sample_df_1.select_dtypes(include=['object']).columns\n",
    "# print(\"Categorical Columns:\", categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the numner of the novsalues in the categorical columns\n",
    "# print(sample_df_1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the columns with leading spaces, if any\n",
    "# columns_with_leading_spaces = [col for col in main_df_2.columns if col.startswith(' ')]\n",
    "\n",
    "# if columns_with_leading_spaces:\n",
    "#     print(\"Columns with leading spaces:\", columns_with_leading_spaces)\n",
    "# else:\n",
    "#     print(\"No columns with leading spaces found.\")\n",
    "\n",
    "# # Remove leading space character in all feature names\n",
    "# # main_df_2.rename(columns=lambda x: x.lstrip(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_744746/2239558539.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sample_df_1['ANOMALY'].fillna(median_anomaly, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# sample_df_1 = sample_df_1.drop(['ANOMALY', 'IPV4_SRC_ADDR'] , axis=1)\n",
    "median_anomaly = sample_df_1['ANOMALY'].median()\n",
    "sample_df_1['ANOMALY'].fillna(median_anomaly, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_744746/2012478311.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sample_df_1['ALERT'].fillna('Normal', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "sample_df_1['ALERT'].fillna('Normal', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extract subsample of data: \n",
      "ALERT\n",
      "Normal               15049330\n",
      "Denial of Service     5138973\n",
      "Port Scanning         4170194\n",
      "Malware                   571\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract subsample of data\n",
    "print(\" Extract subsample of data: \")\n",
    "print(sample_df_1['ALERT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_columns = ['PROTOCOL_MAP', 'IPV4_DST_ADDR', 'IPV4_SRC_ADDR']\n",
    "\n",
    "# Create a copy of the original DataFrame to avoid modifying the original data\n",
    "df_encoded = sample_df_1.copy()\n",
    "\n",
    "# Initialize the OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "df_encoded[categorical_columns] = ordinal_encoder.fit_transform(sample_df_1[categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fill nan values\n",
    "# print(sample_df_1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Dataset Overview:\n"
     ]
    }
   ],
   "source": [
    "print(\"Sampled Dataset Overview:\")\n",
    "\n",
    "dropped_df = ['ALERT']\n",
    "X = df_encoded.drop(dropped_df, axis=1)\n",
    "y = df_encoded['ALERT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples before training: 19487254\n"
     ]
    }
   ],
   "source": [
    "# Initial train test split set and split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Initialize the MLPClassifier\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(\n",
    "    10,), max_iter=1000, random_state=42)\n",
    "\n",
    "# Print the number of samples before training\n",
    "print(\"Number of samples before training:\", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the Decision Tree model\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Print the number of samples after training\n",
    "print(\"Number of samples after training:\", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_encoded = nn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred_encoded)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Classificaiton Report \n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Confusion matrix with string labels\n",
    "conf_mat = confusion_matrix(y_test, y_pred_encoded)\n",
    "\n",
    "# Get unique class labels from y_test and y_pred_encoded\n",
    "unique_labels = np.unique(np.concatenate((y_test, y_pred_encoded)))\n",
    "\n",
    "# Plotting the Confusion Matrix with class labels\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from the first hidden layer\n",
    "weights_first_hidden_layer = nn_model.coefs_[0]\n",
    "\n",
    "# Calculate feature importance based on the weights\n",
    "feature_importance = np.abs(weights_first_hidden_layer).mean(axis=0)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_dict = dict(zip(X.columns.values, feature_importance))\n",
    "sorted_importance = dict(\n",
    "    sorted(importance_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Visualize the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(sorted_importance.values()), y=list(\n",
    "    sorted_importance.keys()), palette='mako')\n",
    "plt.xlabel('Average Weight Magnitude in First Hidden Layer')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.title('Feature Importance in MLPClassifier')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
