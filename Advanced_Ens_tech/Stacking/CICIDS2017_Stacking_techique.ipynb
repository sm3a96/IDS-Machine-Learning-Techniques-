{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_214512/1436368134.py:6: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extract subsample of data: \n",
      "Label\n",
      "BENIGN                        2096484\n",
      "DoS Hulk                       172849\n",
      "DDoS                           128016\n",
      "PortScan                        90819\n",
      "DoS GoldenEye                   10286\n",
      "FTP-Patator                      5933\n",
      "DoS slowloris                    5385\n",
      "DoS Slowhttptest                 5228\n",
      "SSH-Patator                      3219\n",
      "Bot                              1953\n",
      "Web Attack � Brute Force         1470\n",
      "Web Attack � XSS                  652\n",
      "Infiltration                       36\n",
      "Web Attack � Sql Injection         21\n",
      "Heartbleed                         11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*- \n",
    "# pip install pandas numpy matplotlib seaborn scikit-learn sympy\n",
    "# This code is applicable to the CICIDS2017 dataset. \n",
    "# implemented Advanced Ensemble techniques: Stacking \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# file_names = ['/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/IDS_Datasets/CICIDS2017_Dataset/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
    "#                '/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/IDS_Datasets/CICIDS2017_Dataset/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "#                  '/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/IDS_Datasets/CICIDS2017_Dataset/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "#                  '/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/IDS_Datasets/CICIDS2017_Dataset/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv',\n",
    "#                  '/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/IDS_Datasets/CICIDS2017_Dataset/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "#                  '/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/IDS_Datasets/CICIDS2017_Dataset/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "#                  '/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/IDS_Datasets/CICIDS2017_Dataset/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "#                  '/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/IDS_Datasets/CICIDS2017_Dataset/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv']\n",
    "\n",
    "# dataframes = []\n",
    "\n",
    "# for file_name in file_names:\n",
    "#     df = pd.read_csv(file_name)\n",
    "#     dataframes.append(df)\n",
    "\n",
    "# # Concatenate all DataFrames into a single DataFrame\n",
    "# combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "# # Save the combined DataFrame to a CSV file\n",
    "# combined_df.to_csv('combined_dataset.csv', index=False)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/home/ibibers@ads.iu.edu/Intrusion_Detection_System_IDS/Advanced_Ensemble_techniques/Stacking/CICIDS2017_combined_dataset.csv')\n",
    "\n",
    "# Drop duplicates: same observation but multiple occurences\n",
    "main_df_1 = df.drop_duplicates(keep='first')\n",
    "\n",
    "# Drop columns that have just one unique value, the model wont learn form columns like this.\n",
    "one_value = main_df_1.columns[main_df_1.nunique() == 1]\n",
    "main_df_2 = main_df_1.drop(columns = one_value, axis=1)\n",
    "\n",
    "# Fill nan values\n",
    "main_df_2['Flow Bytes/s'] = main_df_2['Flow Bytes/s'].fillna(main_df_2['Flow Bytes/s'].mean())\n",
    "\n",
    "# Remove leading space character in all feature names\n",
    "main_df_2.rename(columns=lambda x: x.lstrip(), inplace=True)\n",
    "\n",
    "# Checking numbers of missing values on the df\n",
    "# main_df_2.isna().sum().sum()\n",
    "\n",
    "sample_df_1 = main_df_2.copy()\n",
    "\n",
    "# Extract subsample of data\n",
    "print (\" Extract subsample of data: \")\n",
    "print (sample_df_1['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extract subsample of data: \n",
      "Label\n",
      "0     2096484\n",
      "4      172849\n",
      "2      128016\n",
      "10      90819\n",
      "3       10286\n",
      "7        5933\n",
      "6        5385\n",
      "5        5228\n",
      "11       3219\n",
      "1        1953\n",
      "12       1470\n",
      "14        652\n",
      "9          36\n",
      "13         21\n",
      "8          11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Label encoding to convert categorical data to numerical data\n",
    "le = LabelEncoder()\n",
    "sample_df_1['Label'] = le.fit_transform(sample_df_1['Label'])\n",
    "\n",
    "# Extract subsample of data\n",
    "print (\" Extract subsample of data: \")\n",
    "print (sample_df_1['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Dataset Overview:\n"
     ]
    }
   ],
   "source": [
    "# sample_df_1.shape\n",
    "# sample_df_1.info()\n",
    "\n",
    "# Take a random sample of the dataset\n",
    "# sample_df_1 = sample_df_1.sample(frac=0.01, random_state=42)\n",
    "\n",
    "print(\"Sampled Dataset Overview:\")\n",
    "# print(sample_df_2.head())\n",
    "# print (sample_df_2.info())\n",
    "\n",
    "dropped_df = ['Label', 'Flow Packets/s', 'Flow Bytes/s']\n",
    "X = sample_df_1.drop( dropped_df, axis=1) \n",
    "y = sample_df_1['Label']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples before training: 2017889\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the models\n",
    "rf_model = RandomForestClassifier()\n",
    "nn_model = MLPClassifier()\n",
    "lr_model = LogisticRegression()\n",
    "dst_model = DecisionTreeClassifier() \n",
    "\n",
    "# Print the number of samples before training\n",
    "print(\"Number of samples before training:\", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibibers@ads.iu.edu/micromamba/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after training: 2017889\n"
     ]
    }
   ],
   "source": [
    "# Train the models\n",
    "rf_model.fit(X_train, y_train)\n",
    "nn_model.fit(X_train, y_train)\n",
    "lr_model.fit(X_train, y_train)\n",
    "dst_model.fit(X_train, y_train)\n",
    "\n",
    "# Print the number of samples after training\n",
    "print(\"Number of samples after training:\", len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions models\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "nn_pred = nn_model.predict(X_test)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "dst_pred = dst_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a meta-model (e.g., Logistic Regression)\n",
    "meta_model = DecisionTreeClassifier()\n",
    "# meta_model = LogisticRegression()\n",
    "# meta_model = MLPClassifier()\n",
    "# meta_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack models using StackingClassifier\n",
    "stacked_model = StackingClassifier(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('nn', nn_model),\n",
    "    ('lr', lr_model),\n",
    "    ('dst', dst_model),\n",
    "    # ('svm', svm_model)\n",
    "], final_estimator=meta_model)\n",
    "\n",
    "# Train the stacked model\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the stacked model\n",
    "stacked_pred__encoded = stacked_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the encoded labels to the original labels by using inverse_transform \n",
    "y_pred = le.inverse_transform(stacked_pred__encoded)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "\n",
    "# Create a mapping between numerical labels and their corresponding real labels\n",
    "label_mapping = {label: le.inverse_transform([label])[0] for label in set(y_test)}\n",
    "print (label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_labels, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(y_test_labels, y_pred , labels = le.classes_)\n",
    "\n",
    "# Plotting the Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,  xticklabels=le.classes_.tolist(), yticklabels=le.classes_.tolist()) \n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from base models \n",
    "base_models_importances = {}\n",
    "\n",
    "for name, model in stacked_model.named_estimators_.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        base_models_importances[name] = model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame(base_models_importances, index=X.columns)\n",
    "\n",
    "# Calculate average feature importance across base models\n",
    "average_importance = importance_df.mean(axis=1)\n",
    "\n",
    "# Sort features by average importance\n",
    "sorted_importance = average_importance.sort_values(ascending=False)\n",
    "\n",
    "# Visualize the feature importances\n",
    "plt.figure(figsize=(14, 20))\n",
    "sns.barplot(x=sorted_importance.values, y=sorted_importance.index, palette='mako')\n",
    "plt.xlabel('Average Importance Value')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.title('Average Feature Importance in StackingClassifier')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
